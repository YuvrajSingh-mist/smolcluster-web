<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Smolcluster - A distributed deep learning library for training neural networks across heterogeneous hardware using PyTorch.">
  <meta name="keywords" content="Smolcluster, Distributed Training, Deep Learning, PyTorch, Model Parallelism">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Smolcluster - Distributed Deep Learning Library</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/YuvrajSingh-mist/smolcluster">
      <span class="icon">
          <i class="fab fa-github"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Smolcluster</h1>
          <h2 class="subtitle is-3">Distributed Deep Learning Library</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Training neural networks across heterogeneous hardware with PyTorch
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- GitHub Link. -->
              <span class="link-block">
                <a href="https://github.com/YuvrajSingh-mist/smolcluster"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
              <!-- Documentation Link. -->
              <span class="link-block">
                <a href="https://github.com/YuvrajSingh-mist/smolcluster#documentation"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-book"></i>
                  </span>
                  <span>Documentation</span>
                </a>
              </span>
            </div>

          </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <strong>Smolcluster</strong> is a distributed deep learning library designed for training neural networks 
            across heterogeneous hardware using PyTorch and socket-based communication. It enables researchers and developers 
            to leverage multiple machines with different capabilities for distributed training and inference.
          </p>
          <p>
            The library supports various distributed training algorithms including Elastic Distributed Parallelism (EDP), 
            Synchronous Parameter Server (SyncPS), and Model Parallelism. It can run on diverse hardware including Mac minis, 
            Raspberry Pis, MacBooks, and Windows machines.
          </p>
        </div>
      </div>
    </div>
    <!--/ Overview. -->

    <!-- Architecture Image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Cluster Architecture</h2>
        <img src="./static/images/architecture.png" alt="Smolcluster Architecture Diagram" style="max-width: 100%; height: auto; margin: 20px 0;">
      </div>
    </div>
    <!--/ Architecture Image. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Features. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Features</h2>

        <div class="content has-text-justified">
          <h3 class="title is-4">üîÑ Distributed Training Algorithms</h3>
          <ul>
            <li><strong>Elastic Distributed Parallelism (EDP)</strong> - Asynchronous data parallelism with stale gradient tolerance, ideal for heterogeneous clusters</li>
            <li><strong>Synchronous Parameter Server (SyncPS)</strong> - Synchronous data parallelism with barrier coordination for homogeneous clusters</li>
            <li><strong>Model Parallelism (MP)</strong> - Layer-wise model distribution perfect for large models and inference serving</li>
          </ul>

          <h3 class="title is-4">üñ•Ô∏è Hardware Support</h3>
          <p>
            Train across heterogeneous hardware including Mac minis, Raspberry Pis, MacBooks, and Windows machines. 
            The framework intelligently handles different hardware capabilities and network latencies.
          </p>

          <h3 class="title is-4">ü§ñ Model Support</h3>
          <p>
            Built-in support for MNIST, GPT-2, and custom neural networks. Includes distributed inference with 
            model parallelism and streaming token generation for language models.
          </p>

          <h3 class="title is-4">üìä Monitoring & Logging</h3>
          <ul>
            <li><strong>Grafana + Loki</strong> - Centralized log aggregation with real-time queries across all nodes</li>
            <li><strong>Weights & Biases Integration</strong> - Automatic tracking of training metrics, gradient norms, and hardware utilization</li>
            <li><strong>Web Interface</strong> - React-based chat UI for GPT inference</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Features. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Demo Video. -->
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Demo</h2>
        <div class="content">
          <video controls style="max-width: 100%; height: auto; margin: 20px 0;">
            <source src="./static/videos/inference_mp.mov" type="video/quicktime">
            Your browser does not support the video tag.
          </video>
          <div class="content has-text-left" style="max-width: 800px; margin: 0 auto;">
            <p><strong>Distributed GPT-2 Inference with Model Parallelism:</strong></p>
            <ul>
              <li>Model: GPT-2 (117M parameters)</li>
              <li>Hardware: iPad client + 2√ó Mac Mini M4 (2025)</li>
              <li>Algorithm: Model Parallelism with layer distribution</li>
              <li>Demo: Real-time streaming token generation across distributed layers</li>
              <li>Workflow: User prompts from iPad ‚Üí activations forwarded between Mac Minis ‚Üí tokens streamed back</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <!--/ Demo Video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Getting Started. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Getting Started</h2>

        <div class="content">
          <p><strong>Note:</strong> Smolcluster requires a distributed hardware setup and network configuration before you can begin training. This is not a straightforward installation.</p>
          
          <h4>Prerequisites</h4>
          <p>Before using Smolcluster, you need to set up your distributed cluster:</p>
          <ul>
            <li><strong>Hardware Setup:</strong> Configure your machines (Mac minis, Raspberry Pis, GPUs, etc.)</li>
            <li><strong>Network Configuration:</strong>
              <ul>
                <li>Mac minis: Thunderbolt connections and network bridges</li>
                <li>Raspberry Pi/GPUs: Ethernet connections</li>
                <li>SSH setup with proper gateways and key authentication</li>
              </ul>
            </li>
            <li><strong>Cluster Configuration:</strong> YAML configuration files for your specific topology</li>
          </ul>

          <h4>Installation</h4>
          <p>Once your cluster is properly configured:</p>
          <pre><code># Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and install
git clone https://github.com/YuvrajSingh-mist/smolcluster.git
cd smolcluster
uv sync</code></pre>

          <p><strong>Important:</strong> Please refer to the <a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/main/docs/setup_cluster.md">Cluster Setup Guide</a> for detailed hardware setup, networking configuration, and troubleshooting before attempting to run training scripts.</p>
        </div>
      </div>
    </div>
    <!--/ Quick Start. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Technical Details. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Technical Details</h2>

        <div class="content has-text-justified">
          <p>
            Smolcluster implements a distributed training system designed from the ground up for heterogeneous hardware. 
            The library supports multiple distributed training paradigms, each optimized for different cluster configurations 
            and network topologies.
          </p>

          <h3 class="title is-4">Communication Infrastructure</h3>
          <ul>
            <li><strong>Socket-based Communication</strong> - Raw TCP sockets for reliable, low-level control over gradient and activation transfers between nodes. No dependency on MPI or specialized networking libraries.</li>
            <li><strong>Pickle Serialization</strong> - PyTorch tensors serialized with pickle for efficient network transmission, with optional gradient quantization for bandwidth reduction.</li>
            <li><strong>Hybrid Network Support</strong> - Handles complex topologies mixing Thunderbolt fabric (10Gbps+) and Ethernet edge connections (1Gbps), with proper routing and gateway configuration.</li>
            <li><strong>Asynchronous I/O</strong> - Non-blocking socket operations enable workers to compute while waiting for network transfers in EDP mode.</li>
          </ul>

          <h3 class="title is-4">Distributed Training Modes</h3>
          <ul>
            <li><strong>Elastic Distributed Parallelism (EDP)</strong> - Workers train independently with stale gradient tolerance. The parameter server accepts gradients from any model version, making it resilient to stragglers and network latency variance. Workers periodically pull the latest weights without synchronization barriers.</li>
            <li><strong>Synchronous Parameter Server (SyncPS)</strong> - Barrier-based coordination where the server waits for all workers to submit gradients before updating. Uses Polyak averaging and synchronous weight broadcasts for faster convergence on homogeneous clusters.</li>
            <li><strong>Model Parallelism</strong> - Sequential layer distribution across nodes with activation forwarding. Enables training and inference of models exceeding single-device memory. Each worker holds a subset of layers and forwards activations to the next rank.</li>
          </ul>

          <h3 class="title is-4">Data Management</h3>
          <ul>
            <li><strong>Automatic Data Partitioning</strong> - Dataset automatically sharded across workers based on global rank and world size, ensuring no data overlap.</li>
            <li><strong>Deterministic Shuffling</strong> - Seeded random number generators ensure reproducible data ordering across runs.</li>
            <li><strong>Streaming Support</strong> - Memory-efficient data loading for large datasets with PyTorch DataLoader integration.</li>
          </ul>

          <h3 class="title is-4">Fault Tolerance & Monitoring</h3>
          <ul>
            <li><strong>Checkpointing</strong> - Periodic model snapshots with configurable intervals. Supports resuming training from the latest checkpoint after failures.</li>
            <li><strong>Distributed Logging</strong> - Grafana + Loki stack aggregates logs from all nodes in real-time. Promtail agents on each machine forward structured logs to a central Loki instance.</li>
            <li><strong>Weights & Biases Integration</strong> - Automatic logging of training metrics, gradient norms, per-layer statistics, and system metrics (GPU utilization, memory usage, network throughput).</li>
            <li><strong>Timeout Handling</strong> - Configurable timeouts prevent deadlocks when workers fail or network partitions occur.</li>
          </ul>

          <h3 class="title is-4">Performance Optimizations</h3>
          <ul>
            <li><strong>Gradient Quantization</strong> - Optional 8-bit quantization reduces gradient transfer size by 4x with minimal accuracy impact.</li>
            <li><strong>CPU-based Computation</strong> - Designed to utilize CPU cores on commodity hardware (Mac minis, Raspberry Pis) rather than requiring GPUs.</li>
            <li><strong>Mixed Precision Training</strong> - FP16 automatic mixed precision support for compatible hardware to accelerate training.</li>
            <li><strong>Gradient Accumulation</strong> - Simulates larger batch sizes by accumulating gradients over multiple micro-batches before updating.</li>
          </ul>

          <p><strong>See the architecture diagram above for a visual representation of the cluster topology.</strong></p>
        </div>
      </div>
    </div>
    <!--/ Technical Details. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Documentation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Documentation</h2>

        <div class="content">
          <p>Comprehensive guides to help you get the most out of Smolcluster:</p>
          <ul>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/main/docs/configuration.md">Configuration Guide</a> - Cluster and model configuration</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/main/docs/training.md">Training Guide</a> - Training algorithms and usage</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/main/docs/logging.md">Logging Setup</a> - Grafana + Loki distributed logging</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/main/docs/setup_cluster.md">Cluster Setup</a> - Hardware setup and networking</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/main/docs/inference.md">Inference Guide</a> - Model parallelism inference</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Documentation. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- License. -->
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">License</h2>
        <div class="content">
          <p>Smolcluster is released under the MIT License.</p>
          <p>Contributions are welcome! Visit the <a href="https://github.com/YuvrajSingh-mist/smolcluster">GitHub repository</a> to get involved.</p>
        </div>
      </div>
    </div>
    <!--/ License. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/YuvrajSingh-mist/smolcluster" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Smolcluster - Distributed Deep Learning Library
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
