<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <title>smolcluster - Distributed Deep Learning Library for Heterogeneous Hardware | PyTorch Training</title>
  <meta name="title" content="smolcluster - Distributed Deep Learning Library for Heterogeneous Hardware | PyTorch Training">
  <meta name="description" content="smolcluster: Open-source distributed deep learning library for training neural networks across heterogeneous hardware (Mac minis, Raspberry Pi, GPUs). Features EDP, SyncPS, Model Parallelism, GPT-2 inference, and socket-based PyTorch communication. Perfect for distributed AI training.">
  <meta name="keywords" content="smolcluster, distributed deep learning, distributed training, PyTorch distributed, heterogeneous hardware, model parallelism, data parallelism, EDP, elastic distributed parallelism, parameter server, distributed inference, GPT-2 distributed, Mac mini cluster, Raspberry Pi cluster, distributed AI, neural network training, PyTorch cluster, socket-based communication, distributed PyTorch, heterogeneous cluster training, distributed machine learning, multi-device training, distributed neural networks, PyTorch multi-node, cluster computing, distributed GPT, model parallel inference, async training, sync training">
  <meta name="author" content="Yuvraj Singh">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  <meta name="revisit-after" content="7 days">
  <meta name="theme-color" content="#000000">
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://yuvrajsingh-mist.github.io/smolcluster-web/">
  
  <!-- Sitemap -->
  <link rel="sitemap" type="application/xml" href="/smolcluster-web/sitemap.xml">
  
  <!-- Manifest for PWA -->
  <link rel="manifest" href="/smolcluster-web/manifest.json">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://yuvrajsingh-mist.github.io/smolcluster-web/">
  <meta property="og:title" content="smolcluster - Distributed Deep Learning Library for Heterogeneous Hardware">
  <meta property="og:description" content="Open-source distributed deep learning library for training neural networks across heterogeneous hardware. Features EDP, SyncPS, Model Parallelism, and GPT-2 inference with PyTorch.">
  <meta property="og:image" content="https://yuvrajsingh-mist.github.io/smolcluster-web/static/images/architecture.png">
  <meta property="og:site_name" content="smolcluster">
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://yuvrajsingh-mist.github.io/smolcluster-web/">
  <meta property="twitter:title" content="smolcluster - Distributed Deep Learning Library for Heterogeneous Hardware">
  <meta property="twitter:description" content="Open-source distributed deep learning library for training neural networks across heterogeneous hardware. Features EDP, SyncPS, Model Parallelism, and GPT-2 inference with PyTorch.">
  <meta property="twitter:image" content="https://yuvrajsingh-mist.github.io/smolcluster-web/static/images/architecture.png">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  
  <!-- Favicons -->
  <link rel="icon" type="image/svg+xml" href="./static/images/favicon.svg">
  <link rel="alternate icon" href="./static/images/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="./static/images/apple-touch-icon.png">
  
  <!-- JSON-LD Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "SoftwareApplication",
    "name": "smolcluster",
    "applicationCategory": "DeveloperApplication",
    "applicationSubCategory": "Machine Learning Framework",
    "description": "Distributed deep learning library for training neural networks across heterogeneous hardware using PyTorch. Supports EDP, SyncPS, Model Parallelism, and distributed GPT-2 inference.",
    "operatingSystem": ["macOS", "Linux", "Windows"],
    "offers": {
      "@type": "Offer",
      "price": "0",
      "priceCurrency": "USD"
    },
    "author": {
      "@type": "Person",
      "name": "Yuvraj Singh",
      "url": "https://github.com/YuvrajSingh-mist"
    },
    "url": "https://yuvrajsingh-mist.github.io/smolcluster-web/",
    "downloadUrl": "https://github.com/YuvrajSingh-mist/smolcluster",
    "screenshot": "https://yuvrajsingh-mist.github.io/smolcluster-web/static/images/architecture.png",
    "softwareVersion": "1.0",
    "programmingLanguage": "Python",
    "license": "https://opensource.org/licenses/MIT",
    "keywords": "distributed deep learning, PyTorch, heterogeneous hardware, model parallelism, distributed training, neural networks, machine learning, cluster computing",
    "featureList": [
      "Elastic Distributed Parallelism (EDP)",
      "Synchronous Parameter Server (SyncPS)",
      "Model Parallelism",
      "Distributed Inference",
      "Heterogeneous Hardware Support",
      "Socket-based Communication",
      "Grafana + Loki Logging",
      "Weights & Biases Integration",
      "GPT-2 Distributed Inference"
    ],
    "codeRepository": "https://github.com/YuvrajSingh-mist/smolcluster",
    "sameAs": [
      "https://github.com/YuvrajSingh-mist/smolcluster"
    ]
  }
  </script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/YuvrajSingh-mist/smolcluster">
      <span class="icon">
          <i class="fab fa-github"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" itemprop="name">smolcluster</h1>
          <h2 class="subtitle is-3">Distributed Deep Learning Library for Heterogeneous Hardware</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Training and inference for neural networks across Mac minis, Raspberry Pi, and GPUs with PyTorch
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">\n              <!-- GitHub Link. -->
              <span class="link-block">
                <a href="https://github.com/YuvrajSingh-mist/smolcluster"
                   class="external-link button is-normal is-rounded is-dark"
                   aria-label="View smolcluster on GitHub"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
              <!-- Documentation Link. -->
              <span class="link-block">
                <a href="https://github.com/YuvrajSingh-mist/smolcluster#documentation"
                   class="external-link button is-normal is-rounded is-dark"
                   aria-label="Read smolcluster documentation"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fas fa-book"></i>
                  </span>
                  <span>Documentation</span>
                </a>
              </span>
            </div>

          </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <strong>smolcluster</strong> is a distributed deep learning library designed for training neural networks 
            across heterogeneous hardware using PyTorch and socket-based communication. It enables researchers and developers 
            to leverage multiple machines with different capabilities for distributed training and inference.
          </p>
          <p>
            The library supports various distributed training algorithms including Elastic Distributed Parallelism (EDP), 
            Synchronous Parameter Server (SyncPS), and Model Parallelism. It can run on diverse hardware including Mac minis, 
            Raspberry Pis, MacBooks, and Windows machines.
          </p>
        </div>
      </div>
    </div>
    <!--/ Overview. -->

    <!-- Architecture Image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Cluster Architecture</h2>
        <img src="./static/images/architecture.png" 
             alt="smolcluster distributed deep learning architecture diagram showing heterogeneous hardware cluster with Mac minis, Raspberry Pi, and parameter server topology for PyTorch model parallelism and data parallelism" 
             title="smolcluster Distributed Training Architecture"
             loading="lazy"
             style="max-width: 100%; height: auto; margin: 20px 0;">
      </div>
    </div>
    <!--/ Architecture Image. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Features. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Features</h2>

        <div class="content has-text-justified">
          <h3 class="title is-4">üîÑ Distributed Training Algorithms</h3>
          <ul>
            <li><strong>Elastic Distributed Parallelism (EDP)</strong> - Asynchronous data parallelism with stale gradient tolerance, ideal for heterogeneous clusters</li>
            <li><strong>Synchronous Parameter Server (SyncPS)</strong> - Synchronous data parallelism with barrier coordination for homogeneous clusters</li>
            <li><strong>Model Parallelism (MP)</strong> - Layer-wise model distribution perfect for large models and inference serving</li>
          </ul>

          <h3 class="title is-4">ÔøΩ Distributed Inference</h3>
          <ul>
            <li><strong>Model Parallelism Inference</strong> - Run large language models that exceed single-device memory by distributing layers across multiple nodes</li>
            <li><strong>Streaming Token Generation</strong> - Real-time token-by-token generation with activations forwarded sequentially through distributed layers</li>
            <li><strong>FastAPI Backend</strong> - RESTful API server for easy integration with web and mobile clients (iPad, browsers, etc.)</li>
            <li><strong>Multi-Device Support</strong> - Serve models across heterogeneous hardware (Mac minis, Raspberry Pis) with automatic activation routing</li>
            <li><strong>Interactive Chat Interface</strong> - React-based web UI and iOS Swift app for real-time interaction with distributed models</li>
          </ul>

          <h3 class="title is-4">üñ•Ô∏è Hardware Support</h3>
          <p>
            Train and run inference across heterogeneous hardware including Mac minis, Raspberry Pis, MacBooks, Windows machines, and iPad clients. 
          </p>

          <h3 class="title is-4">ü§ñ Model Support</h3>
          <p>
            Built-in support for MNIST, GPT-2 (117M parameters), and custom neural networks. Supports both training and distributed inference with 
            model parallelism. Streaming token generation enables interactive applications with large language models.
          </p>

          <h3 class="title is-4">üìä Monitoring & Logging</h3>
          <ul>
            <li><strong>Grafana + Loki</strong> - Centralized log aggregation with real-time queries across all nodes</li>
            <li><strong>Weights & Biases Integration</strong> - Automatic tracking of training metrics, gradient norms, and hardware utilization</li>
            <li><strong>Web Interface</strong> - React-based chat UI for GPT inference with real-time streaming responses</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Features. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Demo Video. -->
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Demo</h2>
        <div class="content">
          <div class="publication-video" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; margin: 20px 0;">
            <iframe style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" 
                    src="https://www.youtube.com/embed/Jp0ptkampk8" 
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
            </iframe>
          </div>
          <div class="content has-text-left" style="max-width: 800px; margin: 0 auto;">
            <p><strong>Distributed GPT-2 Inference with Model Parallelism:</strong></p>
            <ul>
              <li>Model: GPT-2 (117M parameters)</li>
              <li>Hardware: iPad client + 2√ó Mac Mini M4 (2025)</li>
              <li>Algorithm: Model Parallelism with layer distribution</li>
              <li>Demo: Real-time streaming token generation across distributed layers</li>
              <li>Workflow: User prompts from iPad ‚Üí activations forwarded between Mac Minis ‚Üí tokens streamed back</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <!--/ Demo Video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Getting Started. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Getting Started</h2>

        <div class="content">
          <p><strong>Note:</strong> Smolcluster requires a distributed hardware setup and network configuration before you can begin training. This is not a straightforward installation.</p>
          
          <h4>Prerequisites</h4>
          <p>Before using Smolcluster, you need to set up your distributed cluster:</p>
          <ul>
            <li><strong>Hardware Setup:</strong> Configure your machines (Mac minis, Raspberry Pis, GPUs, etc.)</li>
            <li><strong>Network Configuration:</strong>
              <ul>
                <li>Mac minis: Thunderbolt connections and network bridges</li>
                <li>Raspberry Pi/GPUs: Ethernet connections</li>
                <li>SSH setup with proper gateways and key authentication</li>
              </ul>
            </li>
            <li><strong>Cluster Configuration:</strong> YAML configuration files for your specific topology</li>
          </ul>

          <h4>Installation</h4>
          <p>Once your cluster is properly configured:</p>
          <pre><code># Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and install
git clone https://github.com/YuvrajSingh-mist/smolcluster.git
cd smolcluster
uv sync</code></pre>

          <p><strong>Important:</strong> Please refer to the <a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/master/docs/setup_cluster.md">Cluster Setup Guide</a> for detailed hardware setup, networking configuration, and troubleshooting before attempting to run training scripts.</p>
        </div>
      </div>
    </div>
    <!--/ Quick Start. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Technical Details. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Technical Details</h2>

        <div class="content has-text-justified">
          <p>
            Smolcluster implements a distributed training system designed from the ground up for heterogeneous hardware. 
            The library supports multiple distributed training paradigms, each optimized for different cluster configurations 
            and network topologies.
          </p>

          <h3 class="title is-4">Communication Infrastructure</h3>
          <ul>
            <li><strong>Socket-based Communication</strong> - Raw TCP sockets for reliable, low-level control over gradient and activation transfers between nodes. No dependency on MPI or specialized networking libraries.</li>
            <li><strong>Pickle Serialization</strong> - PyTorch tensors serialized with pickle for efficient network transmission, with optional gradient quantization for bandwidth reduction.</li>
            <li><strong>Hybrid Network Support</strong> - Handles complex topologies mixing Thunderbolt fabric (10Gbps+) and Ethernet edge connections (1Gbps), with proper routing and gateway configuration.</li>
            <li><strong>Asynchronous I/O</strong> - Non-blocking socket operations enable workers to compute while waiting for network transfers in EDP mode.</li>
          </ul>

          <h3 class="title is-4">Distributed Training Modes</h3>
          <ul>
            <li><strong>Elastic Distributed Parallelism (EDP)</strong> - Workers train independently with stale gradient tolerance. The parameter server accepts gradients from any model version, making it resilient to stragglers and network latency variance. Workers periodically pull the latest weights without synchronization barriers.</li>
            <li><strong>Synchronous Parameter Server (SyncPS)</strong> - Barrier-based coordination where the server waits for all workers to submit gradients before updating. Uses Polyak averaging and synchronous weight broadcasts for faster convergence on homogeneous clusters.</li>
            <li><strong>Model Parallelism</strong> - Sequential layer distribution across nodes with activation forwarding. Enables training and inference of models exceeding single-device memory. Each worker holds a subset of layers and forwards activations to the next rank.</li>
          </ul>

          <h3 class="title is-4">Data Management</h3>
          <ul>
            <li><strong>Automatic Data Partitioning</strong> - Dataset automatically sharded across workers based on global rank and world size, ensuring no data overlap.</li>
            <li><strong>Deterministic Shuffling</strong> - Seeded random number generators ensure reproducible data ordering across runs.</li>
            <li><strong>Streaming Support</strong> - Memory-efficient data loading for large datasets with PyTorch DataLoader integration.</li>
          </ul>

          <h3 class="title is-4">Fault Tolerance & Monitoring</h3>
          <ul>
            <li><strong>Checkpointing</strong> - Periodic model snapshots with configurable intervals. Supports resuming training from the latest checkpoint after failures.</li>
            <li><strong>Distributed Logging</strong> - Grafana + Loki stack aggregates logs from all nodes in real-time. Promtail agents on each machine forward structured logs to a central Loki instance.</li>
            <li><strong>Weights & Biases Integration</strong> - Automatic logging of training metrics, gradient norms, per-layer statistics, and system metrics (GPU utilization, memory usage, network throughput).</li>
            <li><strong>Timeout Handling</strong> - Configurable timeouts prevent deadlocks when workers fail or network partitions occur.</li>
          </ul>

          <h3 class="title is-4">Performance Optimizations</h3>
          <ul>
            <li><strong>Gradient Quantization</strong> - Optional 8-bit quantization reduces gradient transfer size by 4x with minimal accuracy impact.</li>
            <li><strong>CPU-based Computation</strong> - Designed to utilize CPU cores on commodity hardware (Mac minis, Raspberry Pis) rather than requiring GPUs.</li>
            <li><strong>Mixed Precision Training</strong> - FP16 automatic mixed precision support for compatible hardware to accelerate training.</li>
            <li><strong>Gradient Accumulation</strong> - Simulates larger batch sizes by accumulating gradients over multiple micro-batches before updating.</li>
          </ul>

          <p><strong>See the architecture diagram above for a visual representation of the cluster topology.</strong></p>
        </div>
      </div>
    </div>
    <!--/ Technical Details. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Documentation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Documentation</h2>

        <div class="content">
          <p>Comprehensive guides to help you get the most out of Smolcluster:</p>
          <ul>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/master/docs/configuration.md">Configuration Guide</a> - Cluster and model configuration</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/master/docs/training.md">Training Guide</a> - Training algorithms and usage</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/master/docs/logging.md">Logging Setup</a> - Grafana + Loki distributed logging</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/master/docs/setup_cluster.md">Cluster Setup</a> - Hardware setup and networking</li>
            <li><a href="https://github.com/YuvrajSingh-mist/smolcluster/blob/master/docs/inference.md">Inference Guide</a> - Model parallelism inference</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Documentation. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- License. -->
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">License</h2>
        <div class="content">
          <p>Smolcluster is released under the MIT License.</p>
          <p>Contributions are welcome! Visit the <a href="https://github.com/YuvrajSingh-mist/smolcluster">GitHub repository</a> to get involved.</p>
        </div>
      </div>
    </div>
    <!--/ License. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" 
         href="https://github.com/YuvrajSingh-mist/smolcluster" 
         class="external-link"
         aria-label="Visit smolcluster GitHub repository"
         target="_blank"
         rel="noopener noreferrer">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            <strong>smolcluster</strong> - Distributed Deep Learning Library for Heterogeneous Hardware
          </p>
          <p>
            <small>Open-source PyTorch-based framework for distributed training and inference | MIT License</small>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
